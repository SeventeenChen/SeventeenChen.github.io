<!-- build time:Fri Feb 21 2020 00:30:17 GMT+0800 (China Standard Time) --><!DOCTYPE html><html class="theme-next gemini" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="keywords" content="笔记,深度学习,pytorch,python,jupyter notebook,多层感知机,隐藏层,激活函数,梯度,反向传播,"><link rel="alternate" href="/atom.xml" title="Seventeen" type="application/atom+xml"><meta name="description" content="多层感知机多层感知机的基本知识使用多层感知机图像分类的从零开始的实现使用pytorch的简洁实现多层感知机的基本知识深度学习主要关注多层模型。在这里，我们将以多层感知机（multilayer perceptron，MLP）为例，介绍多层神经网络的概念。隐藏层下图展示了一个多层感知机的神经网络图，它含有一个隐藏层，该层中有5个隐藏单元。表达公式具体来说，给定一个小批量样本$\boldsymbol{X"><meta property="og:type" content="article"><meta property="og:title" content="Task1.3 多层感知机"><meta property="og:url" content="http://yoursite.com/2020/02/14/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/index.html"><meta property="og:site_name" content="Seventeen"><meta property="og:description" content="多层感知机多层感知机的基本知识使用多层感知机图像分类的从零开始的实现使用pytorch的简洁实现多层感知机的基本知识深度学习主要关注多层模型。在这里，我们将以多层感知机（multilayer perceptron，MLP）为例，介绍多层神经网络的概念。隐藏层下图展示了一个多层感知机的神经网络图，它含有一个隐藏层，该层中有5个隐藏单元。表达公式具体来说，给定一个小批量样本$\boldsymbol{X"><meta property="og:image" content="https://cdn.kesci.com/upload/image/q5ho684jmh.png"><meta property="og:image" content="https://cdn.kesci.com/rt_upload/070825B6A382411DA5BD7D14E67E8D54/q5oe5b289s.png"><meta property="og:image" content="https://cdn.kesci.com/rt_upload/BFB05150DBD1474D9A9ECCB9CDF1DD39/q5oe5bgivv.png"><meta property="og:image" content="https://cdn.kesci.com/rt_upload/68FCB4E8142144458F13128B370D1C91/q5oe5ctfoa.png"><meta property="og:image" content="https://cdn.kesci.com/rt_upload/878C7B8823304F72860965E119A21412/q5oe5cbbzo.png"><meta property="og:image" content="https://cdn.kesci.com/rt_upload/92D16076309F42169482834C0B6ABB24/q5oe5c1jlq.png"><meta property="og:image" content="https://cdn.kesci.com/rt_upload/CB16F4B33E664E14BCE8E52D8B37C47F/q5oe5cwaur.png"><meta property="article:published_time" content="2020-02-14T07:42:58.425Z"><meta property="article:modified_time" content="2020-02-15T14:32:06.443Z"><meta property="article:author" content="Seventeen Chen"><meta property="article:tag" content="笔记"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="pytorch"><meta property="article:tag" content="python"><meta property="article:tag" content="jupyter notebook"><meta property="article:tag" content="多层感知机"><meta property="article:tag" content="隐藏层"><meta property="article:tag" content="激活函数"><meta property="article:tag" content="梯度"><meta property="article:tag" content="反向传播"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.kesci.com/upload/image/q5ho684jmh.png"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.4",sidebar:{position:"left",offset:12,b2t:!0,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2020/02/14/多层感知机/"><title>Task1.3 多层感知机 | Seventeen</title><meta name="generator" content="Hexo 4.2.0"></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><a href="https://github.com/SeventeenChen" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#70B7FD;color:#fff;position:absolute;top:0;border:0;right:0" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Seventeen</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">All is well</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/14/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Seventeen Chen"><meta itemprop="description" content=""><meta itemprop="image" content="/uploads/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Seventeen"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Task1.3 多层感知机</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-14T15:42:58+08:00">2020-02-14 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%8A%A8%E6%89%8B%E5%AD%A6DL-pytorch-%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">动手学DL (pytorch) 笔记</span> </a></span></span><span class="post-meta-divider">|</span> <span class="page-pv"><i class="fa fa-file-o"></i> <span class="busuanzi-value" id="busuanzi_value_page_pv"></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h1><ol><li>多层感知机的基本知识</li><li>使用多层感知机图像分类的从零开始的实现</li><li>使用pytorch的简洁实现</li></ol><h2 id="多层感知机的基本知识"><a href="#多层感知机的基本知识" class="headerlink" title="多层感知机的基本知识"></a>多层感知机的基本知识</h2><p>深度学习主要关注多层模型。在这里，我们将以多层感知机（multilayer perceptron，MLP）为例，介绍多层神经网络的概念。</p><h3 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h3><p>下图展示了一个多层感知机的神经网络图，它含有一个隐藏层，该层中有5个隐藏单元。</p><p><img src="https://cdn.kesci.com/upload/image/q5ho684jmh.png" alt="Image Name"></p><h3 id="表达公式"><a href="#表达公式" class="headerlink" title="表达公式"></a>表达公式</h3><p>具体来说，给定一个小批量样本$\boldsymbol{X} \in \mathbb{R}^{n \times d}$，其批量大小为$n$，输入个数为$d$。假设多层感知机只有一个隐藏层，其中隐藏单元个数为$h$。记隐藏层的输出（也称为隐藏层变量或隐藏变量）为$\boldsymbol{H}$，有$\boldsymbol{H} \in \mathbb{R}^{n \times h}$。因为隐藏层和输出层均是全连接层，可以设隐藏层的权重参数和偏差参数分别为$\boldsymbol{W}_h \in \mathbb{R}^{d \times h}$和 $\boldsymbol{b}_h \in \mathbb{R}^{1 \times h}$，输出层的权重和偏差参数分别为$\boldsymbol{W}_o \in \mathbb{R}^{h \times q}$和$\boldsymbol{b}_o \in \mathbb{R}^{1 \times q}$。</p><p>我们先来看一种含单隐藏层的多层感知机的设计。其输出$\boldsymbol{O} \in \mathbb{R}^{n \times q}$的计算为</p><p>$$<br>\begin{aligned} \boldsymbol{H} &amp;= \boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h,\ \boldsymbol{O} &amp;= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o, \end{aligned}<br>$$</p><p>也就是将隐藏层的输出直接作为输出层的输入。如果将以上两个式子联立起来，可以得到</p><p>$$<br>\boldsymbol{O} = (\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h)\boldsymbol{W}_o + \boldsymbol{b}_o = \boldsymbol{X} \boldsymbol{W}_h\boldsymbol{W}_o + \boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o.<br>$$</p><p>从联立后的式子可以看出，虽然神经网络引入了隐藏层，却依然等价于一个单层神经网络：其中输出层权重参数为$\boldsymbol{W}_h\boldsymbol{W}_o$，偏差参数为$\boldsymbol{b}_h \boldsymbol{W}_o + \boldsymbol{b}_o$。不难发现，即便再添加更多的隐藏层，以上设计依然只能与仅含输出层的单层神经网络等价。</p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>上述问题的根源在于全连接层只是对数据做仿射变换（affine transformation），而多个仿射变换的叠加仍然是一个仿射变换。解决问题的一个方法是引入非线性变换，例如对隐藏变量使用按元素运算的非线性函数进行变换，然后再作为下一个全连接层的输入。这个非线性函数被称为激活函数（activation function）。</p><p>下面我们介绍几个常用的激活函数：</p><h4 id="ReLU函数"><a href="#ReLU函数" class="headerlink" title="ReLU函数"></a>ReLU函数</h4><p>ReLU（rectified linear unit）函数提供了一个很简单的非线性变换。给定元素$x$，该函数定义为</p><p>$$<br>\text{ReLU}(x) = \max(x, 0).<br>$$</p><p>可以看出，ReLU函数只保留正数元素，并将负数元素清零。为了直观地观察这一非线性变换，我们先定义一个绘图函数xyplot。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"/home/kesci/input"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line">print(torch.__version__)</span><br></pre></td></tr></table></figure><pre><code>1.3.0</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xyplot</span><span class="params">(x_vals, y_vals, name)</span>:</span></span><br><span class="line">    <span class="comment"># d2l.set_figsize(figsize=(5, 2.5))</span></span><br><span class="line">    plt.plot(x_vals.detach().numpy(), y_vals.detach().numpy()) <span class="comment"># 参数x_vals &amp; y_vals 不需要计算梯度</span></span><br><span class="line">    plt.xlabel(<span class="string">'x'</span>) </span><br><span class="line">    plt.ylabel(name + <span class="string">'(x)'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">-8.0</span>, <span class="number">8.0</span>, <span class="number">0.1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x.relu()</span><br><span class="line">xyplot(x, y, <span class="string">'relu'</span>)</span><br></pre></td></tr></table></figure><img src="https://cdn.kesci.com/rt_upload/070825B6A382411DA5BD7D14E67E8D54/q5oe5b289s.png"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y.sum().backward() <span class="comment"># 构造标量计算梯度</span></span><br><span class="line">xyplot(x, x.grad, <span class="string">'grad of relu'</span>) <span class="comment"># relu函数的梯度图</span></span><br></pre></td></tr></table></figure><img src="https://cdn.kesci.com/rt_upload/BFB05150DBD1474D9A9ECCB9CDF1DD39/q5oe5bgivv.png"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># print(x,y)</span></span><br><span class="line">print(x.grad)</span><br></pre></td></tr></table></figure><pre><code>tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])</code></pre><h4 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h4><p>sigmoid函数可以将元素的值变换到0和1之间：</p><p>$$<br>\text{sigmoid}(x) = \frac{1}{1 + \exp(-x)}.<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = x.sigmoid()</span><br><span class="line">xyplot(x, y, <span class="string">'sigmoid'</span>)</span><br></pre></td></tr></table></figure><img src="https://cdn.kesci.com/rt_upload/68FCB4E8142144458F13128B370D1C91/q5oe5ctfoa.png"><p>依据链式法则，sigmoid函数的导数</p><p>$$<br>\text{sigmoid}’(x) = \text{sigmoid}(x)\left(1-\text{sigmoid}(x)\right).<br>$$</p><p>下面绘制了sigmoid函数的导数。当输入为0时，sigmoid函数的导数达到最大值0.25；当输入越偏离0时，sigmoid函数的导数越接近0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y.sum().backward()</span><br><span class="line">xyplot(x, x.grad, <span class="string">'grad of sigmoid'</span>)</span><br></pre></td></tr></table></figure><img src="https://cdn.kesci.com/rt_upload/878C7B8823304F72860965E119A21412/q5oe5cbbzo.png"><h4 id="tanh函数"><a href="#tanh函数" class="headerlink" title="tanh函数"></a>tanh函数</h4><p>tanh（双曲正切）函数可以将元素的值变换到-1和1之间：</p><p>$$<br>\text{tanh}(x) = \frac{1 - \exp(-2x)}{1 + \exp(-2x)}.<br>$$</p><p>我们接着绘制tanh函数。当输入接近0时，tanh函数接近线性变换。虽然该函数的形状和sigmoid函数的形状很像，但tanh函数在坐标系的原点上对称。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = x.tanh()</span><br><span class="line">xyplot(x, y, <span class="string">'tanh'</span>)</span><br></pre></td></tr></table></figure><img src="https://cdn.kesci.com/rt_upload/92D16076309F42169482834C0B6ABB24/q5oe5c1jlq.png"><p>依据链式法则，tanh函数的导数</p><p>$$<br>\text{tanh}’(x) = 1 - \text{tanh}^2(x).<br>$$</p><p>下面绘制了tanh函数的导数。当输入为0时，tanh函数的导数达到最大值1；当输入越偏离0时，tanh函数的导数越接近0。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x.grad.zero_()</span><br><span class="line">y.sum().backward()</span><br><span class="line">xyplot(x, x.grad, <span class="string">'grad of tanh'</span>)</span><br></pre></td></tr></table></figure><img src="https://cdn.kesci.com/rt_upload/CB16F4B33E664E14BCE8E52D8B37C47F/q5oe5cwaur.png"><h3 id="关于激活函数的选择"><a href="#关于激活函数的选择" class="headerlink" title="关于激活函数的选择"></a>关于激活函数的选择</h3><p>ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。</p><p>用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。</p><p>在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。</p><p>在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。</p><h3 id="多层感知机-1"><a href="#多层感知机-1" class="headerlink" title="多层感知机"></a>多层感知机</h3><p>多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。多层感知机的层数和各隐藏层中隐藏单元个数都是超参数。以单隐藏层为例并沿用本节之前定义的符号，多层感知机按以下方式计算输出：</p><p>$$<br>\begin{aligned} \boldsymbol{H} &amp;= \phi(\boldsymbol{X} \boldsymbol{W}_h + \boldsymbol{b}_h),\ \boldsymbol{O} &amp;= \boldsymbol{H} \boldsymbol{W}_o + \boldsymbol{b}_o, \end{aligned}<br>$$</p><p>其中$\phi$表示激活函数。</p><h2 id="多层感知机从零开始的实现"><a href="#多层感知机从零开始的实现" class="headerlink" title="多层感知机从零开始的实现"></a>多层感知机从零开始的实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"/home/kesci/input"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line">print(torch.__version__)</span><br></pre></td></tr></table></figure><pre><code>1.3.0</code></pre><h3 id="获取训练集"><a href="#获取训练集" class="headerlink" title="获取训练集"></a>获取训练集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root=<span class="string">'/home/kesci/input/FashionMNIST2065'</span>)</span><br></pre></td></tr></table></figure><h3 id="定义模型参数"><a href="#定义模型参数" class="headerlink" title="定义模型参数"></a>定义模型参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line"></span><br><span class="line">W1 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_inputs, num_hiddens)), dtype=torch.float)</span><br><span class="line">b1 = torch.zeros(num_hiddens, dtype=torch.float)</span><br><span class="line">W2 = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, (num_hiddens, num_outputs)), dtype=torch.float)</span><br><span class="line">b2 = torch.zeros(num_outputs, dtype=torch.float)</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">    param.requires_grad_(requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h3 id="定义激活函数"><a href="#定义激活函数" class="headerlink" title="定义激活函数"></a>定义激活函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu</span><span class="params">(X)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.max(input=X, other=torch.tensor(<span class="number">0.0</span>))</span><br></pre></td></tr></table></figure><h3 id="定义网络"><a href="#定义网络" class="headerlink" title="定义网络"></a>定义网络</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">net</span><span class="params">(X)</span>:</span></span><br><span class="line">    X = X.view((<span class="number">-1</span>, num_inputs))</span><br><span class="line">    H = relu(torch.matmul(X, W1) + b1)</span><br><span class="line">    <span class="keyword">return</span> torch.matmul(H, W2) + b2</span><br></pre></td></tr></table></figure><h3 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, lr = <span class="number">5</span>, <span class="number">100.0</span></span><br><span class="line"><span class="comment"># def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,</span></span><br><span class="line"><span class="comment">#               params=None, lr=None, optimizer=None):</span></span><br><span class="line"><span class="comment">#     for epoch in range(num_epochs):</span></span><br><span class="line"><span class="comment">#         train_l_sum, train_acc_sum, n = 0.0, 0.0, 0</span></span><br><span class="line"><span class="comment">#         for X, y in train_iter:</span></span><br><span class="line"><span class="comment">#             y_hat = net(X)</span></span><br><span class="line"><span class="comment">#             l = loss(y_hat, y).sum()</span></span><br><span class="line"><span class="comment">#             </span></span><br><span class="line"><span class="comment">#             # 梯度清零</span></span><br><span class="line"><span class="comment">#             if optimizer is not None:</span></span><br><span class="line"><span class="comment">#                 optimizer.zero_grad()</span></span><br><span class="line"><span class="comment">#             elif params is not None and params[0].grad is not None:</span></span><br><span class="line"><span class="comment">#                 for param in params:</span></span><br><span class="line"><span class="comment">#                     param.grad.data.zero_()</span></span><br><span class="line"><span class="comment">#            </span></span><br><span class="line"><span class="comment">#             l.backward()</span></span><br><span class="line"><span class="comment">#             if optimizer is None:</span></span><br><span class="line"><span class="comment">#                 d2l.sgd(params, lr, batch_size)</span></span><br><span class="line"><span class="comment">#             else:</span></span><br><span class="line"><span class="comment">#                 optimizer.step()  # “softmax回归的简洁实现”一节将用到</span></span><br><span class="line"><span class="comment">#             </span></span><br><span class="line"><span class="comment">#             </span></span><br><span class="line"><span class="comment">#             train_l_sum += l.item()</span></span><br><span class="line"><span class="comment">#             train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()</span></span><br><span class="line"><span class="comment">#             n += y.shape[0]</span></span><br><span class="line"><span class="comment">#         test_acc = evaluate_accuracy(test_iter, net)</span></span><br><span class="line"><span class="comment">#         print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span></span><br><span class="line"><span class="comment">#               % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))</span></span><br><span class="line"></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)</span><br></pre></td></tr></table></figure><pre><code>epoch 1, loss 0.0030, train acc 0.715, test acc 0.807
epoch 2, loss 0.0019, train acc 0.822, test acc 0.791
epoch 3, loss 0.0017, train acc 0.844, test acc 0.831
epoch 4, loss 0.0015, train acc 0.857, test acc 0.811
epoch 5, loss 0.0015, train acc 0.864, test acc 0.810</code></pre><h2 id="多层感知机pytorch实现"><a href="#多层感知机pytorch实现" class="headerlink" title="多层感知机pytorch实现"></a>多层感知机pytorch实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"/home/kesci/input"</span>)</span><br><span class="line"><span class="keyword">import</span> d2lzh1981 <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line">print(torch.__version__)</span><br></pre></td></tr></table></figure><pre><code>1.3.0</code></pre><h3 id="初始化模型和各个参数"><a href="#初始化模型和各个参数" class="headerlink" title="初始化模型和各个参数"></a>初始化模型和各个参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_outputs, num_hiddens = <span class="number">784</span>, <span class="number">10</span>, <span class="number">256</span></span><br><span class="line">    </span><br><span class="line">net = nn.Sequential(</span><br><span class="line">        d2l.FlattenLayer(),</span><br><span class="line">        nn.Linear(num_inputs, num_hiddens),</span><br><span class="line">        nn.ReLU(),</span><br><span class="line">        nn.Linear(num_hiddens, num_outputs), </span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> params <span class="keyword">in</span> net.parameters():</span><br><span class="line">    init.normal_(params, mean=<span class="number">0</span>, std=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><h3 id="训练-1"><a href="#训练-1" class="headerlink" title="训练"></a>训练</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,root=<span class="string">'/home/kesci/input/FashionMNIST2065'</span>)</span><br><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, <span class="literal">None</span>, <span class="literal">None</span>, optimizer)</span><br></pre></td></tr></table></figure><pre><code>epoch 1, loss 0.0031, train acc 0.700, test acc 0.742
epoch 2, loss 0.0019, train acc 0.819, test acc 0.804
epoch 3, loss 0.0017, train acc 0.841, test acc 0.827
epoch 4, loss 0.0016, train acc 0.853, test acc 0.836
epoch 5, loss 0.0015, train acc 0.862, test acc 0.788</code></pre><hr></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E7%AC%94%E8%AE%B0/" rel="tag"><i class="fa fa-tag"></i> 笔记</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a> <a href="/tags/pytorch/" rel="tag"><i class="fa fa-tag"></i> pytorch</a> <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a> <a href="/tags/jupyter-notebook/" rel="tag"><i class="fa fa-tag"></i> jupyter notebook</a> <a href="/tags/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA/" rel="tag"><i class="fa fa-tag"></i> 多层感知机</a> <a href="/tags/%E9%9A%90%E8%97%8F%E5%B1%82/" rel="tag"><i class="fa fa-tag"></i> 隐藏层</a> <a href="/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/" rel="tag"><i class="fa fa-tag"></i> 激活函数</a> <a href="/tags/%E6%A2%AF%E5%BA%A6/" rel="tag"><i class="fa fa-tag"></i> 梯度</a> <a href="/tags/%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/" rel="tag"><i class="fa fa-tag"></i> 反向传播</a></div><div class="post-widgets"><div id="needsharebutton-postbottom"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2020/02/14/Softmax%E4%B8%8E%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B/" rel="next" title="Task1.2 softmax分类"><i class="fa fa-chevron-left"></i> Task1.2 softmax分类</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2020/02/15/%E6%96%87%E6%9C%AC%E9%A2%84%E5%A4%84%E7%90%86/" rel="prev" title="Task1.4 文本预处理">Task1.4 文本预处理 <i class="fa fa-chevron-right"></i></a></div></div></footer></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">------ 本文结束------</div></div></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><a href="/"><img class="site-author-image" itemprop="image" src="/uploads/avatar.png" alt="Seventeen Chen"></a><p class="site-author-name" itemprop="name">Seventeen Chen</p><p class="site-description motion-element" itemprop="description">This is my blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">13</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">3</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">54</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/SeventeenChen" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:cherry.oldchen@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://twitter.com/SeventeenChen17" target="_blank" title="Twitter"><i class="fa fa-fw fa-twitter"></i>Twitter</a></span></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#多层感知机"><span class="nav-number">1.</span> <span class="nav-text">多层感知机</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#多层感知机的基本知识"><span class="nav-number">1.1.</span> <span class="nav-text">多层感知机的基本知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#隐藏层"><span class="nav-number">1.1.1.</span> <span class="nav-text">隐藏层</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#表达公式"><span class="nav-number">1.1.2.</span> <span class="nav-text">表达公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#激活函数"><span class="nav-number">1.1.3.</span> <span class="nav-text">激活函数</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ReLU函数"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">ReLU函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sigmoid函数"><span class="nav-number">1.1.3.2.</span> <span class="nav-text">Sigmoid函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#tanh函数"><span class="nav-number">1.1.3.3.</span> <span class="nav-text">tanh函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#关于激活函数的选择"><span class="nav-number">1.1.4.</span> <span class="nav-text">关于激活函数的选择</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多层感知机-1"><span class="nav-number">1.1.5.</span> <span class="nav-text">多层感知机</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多层感知机从零开始的实现"><span class="nav-number">1.2.</span> <span class="nav-text">多层感知机从零开始的实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#获取训练集"><span class="nav-number">1.2.1.</span> <span class="nav-text">获取训练集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#定义模型参数"><span class="nav-number">1.2.2.</span> <span class="nav-text">定义模型参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#定义激活函数"><span class="nav-number">1.2.3.</span> <span class="nav-text">定义激活函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#定义网络"><span class="nav-number">1.2.4.</span> <span class="nav-text">定义网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#定义损失函数"><span class="nav-number">1.2.5.</span> <span class="nav-text">定义损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#训练"><span class="nav-number">1.2.6.</span> <span class="nav-text">训练</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多层感知机pytorch实现"><span class="nav-number">1.3.</span> <span class="nav-text">多层感知机pytorch实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#初始化模型和各个参数"><span class="nav-number">1.3.1.</span> <span class="nav-text">初始化模型和各个参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#训练-1"><span class="nav-number">1.3.2.</span> <span class="nav-text">训练</span></a></li></ol></li></ol></li></ol></div></div></section><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Seventeen Chen</span></div><div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div><span class="post-meta-divider">|</span><div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div><div class="busuanzi-count"><script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="site-uv"><i class="fa fa-user"></i> <span class="busuanzi-value" id="busuanzi_value_site_uv"></span> </span><span class="site-pv"><i class="fa fa-eye"></i> <span class="busuanzi-value" id="busuanzi_value_site_pv"></span></span></div></div></footer><div id="needsharebutton-float"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="true"></script><script type="text/javascript" src="true"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css"><script src="/lib/needsharebutton/needsharebutton.js"></script><script>pbOptions={},pbOptions.iconStyle="default",pbOptions.boxForm="horizontal",pbOptions.position="bottomCenter",pbOptions.networks="Weibo,Wechat,Douban,QQZone,Twitter,Facebook",new needShareButton("#needsharebutton-postbottom",pbOptions),flOptions={},flOptions.iconStyle="box",flOptions.boxForm="horizontal",flOptions.position="middleRight",flOptions.networks="Weibo,Wechat,Douban,QQZone,Twitter,Facebook",new needShareButton("#needsharebutton-float",flOptions)</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="/lib/clipboard/clipboard.js"></script><script type="text/javascript" src="/js/src/custom.js"></script></body></html><!-- rebuild by neat -->