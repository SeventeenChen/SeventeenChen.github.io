<!-- build time:Tue Apr 28 2020 19:14:42 GMT+0800 (China Standard Time) --><!DOCTYPE html><html class="theme-next gemini" lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="theme-color" content="#222"><script src="/lib/pace/pace.min.js?v=1.0.2"></script><link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css"><link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css"><link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4"><link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222"><meta name="keywords" content="笔记,深度学习,pytorch,GRU,LSTM,deep RNN,Bi-directional RNN,深度学习实战,python,jupyter notebook,"><link rel="alternate" href="/atom.xml" title="Seventeen" type="application/atom+xml"><meta name="description" content="GRURNN存在的问题：梯度较容易出现衰减或爆炸（BPTT）⻔控循环神经⽹络：捕捉时间序列中时间步距离较⼤的依赖关系RNN:$$H_{t} &#x3D; ϕ(X_{t}W_{xh} + H_{t-1}W_{hh} + b_{h})$$GRU:$$R_{t} &#x3D; σ(X_tW_{xr} + H_{t−1}W_{hr} + b_r)\Z_{t} &#x3D; σ(X_tW_{xz} + H_{t−1}W_{hz} + b"><meta property="og:type" content="article"><meta property="og:title" content="Task1.9 RNN 进阶"><meta property="og:url" content="http://yoursite.com/2020/02/20/ModernRNN/index.html"><meta property="og:site_name" content="Seventeen"><meta property="og:description" content="GRURNN存在的问题：梯度较容易出现衰减或爆炸（BPTT）⻔控循环神经⽹络：捕捉时间序列中时间步距离较⼤的依赖关系RNN:$$H_{t} &#x3D; ϕ(X_{t}W_{xh} + H_{t-1}W_{hh} + b_{h})$$GRU:$$R_{t} &#x3D; σ(X_tW_{xr} + H_{t−1}W_{hr} + b_r)\Z_{t} &#x3D; σ(X_tW_{xz} + H_{t−1}W_{hz} + b"><meta property="og:image" content="https://cdn.kesci.com/upload/image/q5jjvcykud.png?imageView2/0/w/320/h/320"><meta property="og:image" content="https://cdn.kesci.com/upload/image/q5jk0q9suq.png?imageView2/0/w/640/h/640"><meta property="og:image" content="https://cdn.kesci.com/upload/image/q5jk2bnnej.png?imageView2/0/w/640/h/640"><meta property="og:image" content="https://cdn.kesci.com/upload/image/q5jk3z1hvz.png?imageView2/0/w/320/h/320"><meta property="og:image" content="https://cdn.kesci.com/upload/image/q5j8hmgyrz.png?imageView2/0/w/320/h/320"><meta property="article:published_time" content="2020-02-20T13:38:44.865Z"><meta property="article:modified_time" content="2020-02-20T13:39:54.394Z"><meta property="article:author" content="Seventeen Chen"><meta property="article:tag" content="笔记"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="pytorch"><meta property="article:tag" content="GRU"><meta property="article:tag" content="LSTM"><meta property="article:tag" content="deep RNN"><meta property="article:tag" content="Bi-directional RNN"><meta property="article:tag" content="深度学习实战"><meta property="article:tag" content="python"><meta property="article:tag" content="jupyter notebook"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.kesci.com/upload/image/q5jjvcykud.png?imageView2/0/w/320/h/320"><script type="text/javascript" id="hexo.configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"5.1.4",sidebar:{position:"left",offset:12,back2top:null,enable:!0,sidebar:!0,scrollpercent:!0,onmobile:!1},fancybox:!0,tabs:!0,motion:{enable:!1,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},duoshuo:{userId:"0",author:"博主"},algolia:{applicationID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}}}</script><link rel="canonical" href="http://yoursite.com/2020/02/20/ModernRNN/"><title>Task1.9 RNN 进阶 | Seventeen</title><meta name="generator" content="Hexo 4.2.0"></head><body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans"><div class="container sidebar-position-left page-post-detail"><div class="headband"></div><a href="https://github.com/SeventeenChen" target="_blank" rel="noopener" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#70B7FD;color:#fff;position:absolute;top:0;border:0;right:0" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style><header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-wrapper"><div class="site-meta"><div class="custom-logo-site-title"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">Seventeen</span> <span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">All is well</p></div><div class="site-nav-toggle"><button><span class="btn-bar"></span> <span class="btn-bar"></span> <span class="btn-bar"></span></button></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i><br>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i><br>归档</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i><br>分类</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i><br>标签</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i><br>关于</a></li></ul></nav></div></header><main id="main" class="main"><div class="main-inner"><div class="content-wrap"><div id="content" class="content"><div id="posts" class="posts-expand"><article class="post post-type-normal" itemscope itemtype="http://schema.org/Article"><div class="post-block"><link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/20/ModernRNN/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="name" content="Seventeen Chen"><meta itemprop="description" content=""><meta itemprop="image" content="/uploads/avatar.png"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Seventeen"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">Task1.9 RNN 进阶</h1><div class="post-meta"><span class="post-time"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-02-20T21:38:44+08:00">2020-02-20 </time></span><span class="post-category"><span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-folder-o"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E5%8A%A8%E6%89%8B%E5%AD%A6DL-pytorch-%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">动手学DL (pytorch) 笔记</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h1><p>RNN存在的问题：梯度较容易出现衰减或爆炸（BPTT）<br>⻔控循环神经⽹络：捕捉时间序列中时间步距离较⼤的依赖关系<br><strong>RNN</strong>:</p><p><img src="https://cdn.kesci.com/upload/image/q5jjvcykud.png?imageView2/0/w/320/h/320" alt="Image Name"></p><p>$$<br>H_{t} = ϕ(X_{t}W_{xh} + H_{t-1}W_{hh} + b_{h})<br>$$<br><strong>GRU</strong>:</p><p><img src="https://cdn.kesci.com/upload/image/q5jk0q9suq.png?imageView2/0/w/640/h/640" alt="Image Name"></p><p>$$<br>R_{t} = σ(X_tW_{xr} + H_{t−1}W_{hr} + b_r)\<br>Z_{t} = σ(X_tW_{xz} + H_{t−1}W_{hz} + b_z)\<br>\widetilde{H}<em>t = tanh(X_tW</em>{xh} + (R_t ⊙H_{t−1})W_{hh} + b_h)\<br>H_t = Z_t⊙H_{t−1} + (1−Z_t)⊙\widetilde{H}_t<br>$$<br>• 重置⻔有助于捕捉时间序列⾥短期的依赖关系；<br>• 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。</p><h3 id="载入数据集"><a href="#载入数据集" class="headerlink" title="载入数据集"></a>载入数据集</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.listdir(<span class="string">'/home/kesci/input'</span>)</span><br></pre></td></tr></table></figure><pre><code>[&apos;d2lzh1981&apos;, &apos;houseprices2807&apos;, &apos;jaychou_lyrics4703&apos;, &apos;d2l_jay9460&apos;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"../input/"</span>)</span><br><span class="line"><span class="keyword">import</span> d2l_jay9460 <span class="keyword">as</span> d2l</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line">(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()</span><br></pre></td></tr></table></figure><h3 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line">print(<span class="string">'will use'</span>, device)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span>  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        ts = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=shape), device=device, dtype=torch.float32) <span class="comment">#正态分布 </span></span><br><span class="line">        <span class="comment"># 0均值 var = 0.01</span></span><br><span class="line">        <span class="keyword">return</span> torch.nn.Parameter(ts, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_three</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (_one((num_inputs, num_hiddens)), </span><br><span class="line">                _one((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>))</span><br><span class="line">     </span><br><span class="line">    W_xz, W_hz, b_z = _three()  <span class="comment"># 更新门参数</span></span><br><span class="line">    W_xr, W_hr, b_r = _three()  <span class="comment"># 重置门参数</span></span><br><span class="line">    W_xh, W_hh, b_h = _three()  <span class="comment"># 候选隐藏状态参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> nn.ParameterList([W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q]) <span class="comment"># 11 paras</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_gru_state</span><span class="params">(batch_size, num_hiddens, device)</span>:</span>   <span class="comment">#隐藏状态初始化</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), )</span><br></pre></td></tr></table></figure><pre><code>will use cpu</code></pre><h3 id="GRU模型"><a href="#GRU模型" class="headerlink" title="GRU模型"></a>GRU模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gru</span><span class="params">(inputs, state, params)</span>:</span></span><br><span class="line">    W_xz, W_hz, b_z, W_xr, W_hr, b_r, W_xh, W_hh, b_h, W_hq, b_q = params</span><br><span class="line">    H, = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        Z = torch.sigmoid(torch.matmul(X, W_xz) + torch.matmul(H, W_hz) + b_z)</span><br><span class="line">        R = torch.sigmoid(torch.matmul(X, W_xr) + torch.matmul(H, W_hr) + b_r)</span><br><span class="line">        H_tilda = torch.tanh(torch.matmul(X, W_xh) + R * torch.matmul(H, W_hh) + b_h)</span><br><span class="line">        H = Z * H + (<span class="number">1</span> - Z) * H_tilda</span><br><span class="line">        Y = torch.matmul(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> outputs, (H,)</span><br></pre></td></tr></table></figure><h3 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">160</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">40</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">d2l.train_and_predict_rnn(gru, get_params, init_gru_state, num_hiddens,</span><br><span class="line">                          vocab_size, device, corpus_indices, idx_to_char,</span><br><span class="line">                          char_to_idx, <span class="literal">False</span>, num_epochs, num_steps, lr,</span><br><span class="line">                          clipping_theta, batch_size, pred_period, pred_len,</span><br><span class="line">                          prefixes)</span><br></pre></td></tr></table></figure><pre><code>epoch 40, perplexity 152.268495, time 1.31 sec
 - 分开 我想你你 我不这 我不不 我想你你 我不这 我不不 我想你你 我不这 我不不 我想你你 我不这 我
 - 不分开 我想你你 我不这 我不不 我想你你 我不这 我不不 我想你你 我不这 我不不 我想你你 我不这 我
epoch 80, perplexity 32.902482, time 1.29 sec
 - 分开 一直在人截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快
 - 不分开 你爱我 别你 我想要这样 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 我不要再想 
epoch 120, perplexity 5.031946, time 1.26 sec
 - 分开 一直心酒 你的完空 恨谁风空  没有用双截棍 哼哼哈兮 快使用双截棍 哼哼哈兮 快使用双截棍 哼哼
 - 不分开 爱过走的太快就像龙卷风 不能再能我 再你这这不舍 后知后觉 我跟了这节奏 我该好好生活 不知不觉 
epoch 160, perplexity 1.491664, time 1.31 sec
 - 分开 我想想这样的脑袋有问题 随便说说 其实我早已经猜透看透不想多说 只是我怕眼泪撑不住 不懂 你的黑色
 - 不分开 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生活</code></pre><h3 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens=<span class="number">256</span></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">160</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">40</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line"></span><br><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line">gru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens)</span><br><span class="line">model = d2l.RNNModel(gru_layer, vocab_size).to(device)</span><br><span class="line">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure><pre><code>epoch 40, perplexity 1.018378, time 0.95 sec
 - 分开的玩笑 想通 却又再考倒我 说散 你想很久了吧? 败给你的黑色幽默 说散 你想很久了吧? 我的认真败
 - 不分开暴风圈来不及逃 我不能再想 我不能再想 我不 我不 我不能 爱情走的太快就像龙卷风 不能承受我已无处
epoch 80, perplexity 1.012368, time 0.98 sec
 - 分开的玩笑 想通 却又再考倒我 说散 你想很久了吧? 败给你的黑色幽默 说散 你想很久了吧? 我的认真败
 - 不分开爱玩笑 想通 却又再考倒我 说散 你想很久了吧? 败给你的黑色幽默 说散 你想很久了吧? 我的认真败
epoch 120, perplexity 1.013130, time 0.91 sec
 - 分开的可爱女人 漂亮的让我面红的可爱女人 温柔的让我心疼的可爱女人 透明的让我感动的可爱女人 坏坏的让我
 - 不分开不知不觉 你已经离开我 不知不觉 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该
epoch 160, perplexity 1.008711, time 0.96 sec
 - 分开的可爱女人 漂亮的让我面红的可爱女人 温柔的让我心疼的可爱女人 透明的让我感动的可爱女人 坏坏的让我
 - 不分开始打呼 管家是一只会说法语举止优雅的猪 吸血前会念约翰福音做为弥补 拥有一双蓝色眼睛的凯萨琳公主 专</code></pre><h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>** 长短期记忆long short-term memory **:<br>遗忘门:控制上一时间步的记忆细胞<br>输入门:控制当前时间步的输入<br>输出门:控制从记忆细胞到隐藏状态<br>记忆细胞：⼀种特殊的隐藏状态的信息的流动</p><p><img src="https://cdn.kesci.com/upload/image/q5jk2bnnej.png?imageView2/0/w/640/h/640" alt="Image Name"></p><p>$$<br>I_t = σ(X_tW_{xi} + H_{t−1}W_{hi} + b_i) \<br>F_t = σ(X_tW_{xf} + H_{t−1}W_{hf} + b_f)\<br>O_t = σ(X_tW_{xo} + H_{t−1}W_{ho} + b_o)\<br>\widetilde{C}<em>t = tanh(X_tW</em>{xc} + H_{t−1}W_{hc} + b_c)\<br>C_t = F_t ⊙C_{t−1} + I_t ⊙\widetilde{C}_t\<br>H_t = O_t⊙tanh(C_t)<br>$$</p><h3 id="初始化参数-1"><a href="#初始化参数-1" class="headerlink" title="初始化参数"></a>初始化参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">num_inputs, num_hiddens, num_outputs = vocab_size, <span class="number">256</span>, vocab_size</span><br><span class="line">print(<span class="string">'will use'</span>, device)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_params</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_one</span><span class="params">(shape)</span>:</span></span><br><span class="line">        ts = torch.tensor(np.random.normal(<span class="number">0</span>, <span class="number">0.01</span>, size=shape), device=device, dtype=torch.float32)</span><br><span class="line">        <span class="keyword">return</span> torch.nn.Parameter(ts, requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_three</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (_one((num_inputs, num_hiddens)),</span><br><span class="line">                _one((num_hiddens, num_hiddens)),</span><br><span class="line">                torch.nn.Parameter(torch.zeros(num_hiddens, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>))</span><br><span class="line">    </span><br><span class="line">    W_xi, W_hi, b_i = _three()  <span class="comment"># 输入门参数</span></span><br><span class="line">    W_xf, W_hf, b_f = _three()  <span class="comment"># 遗忘门参数</span></span><br><span class="line">    W_xo, W_ho, b_o = _three()  <span class="comment"># 输出门参数</span></span><br><span class="line">    W_xc, W_hc, b_c = _three()  <span class="comment"># 候选记忆细胞参数</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 输出层参数</span></span><br><span class="line">    W_hq = _one((num_hiddens, num_outputs))</span><br><span class="line">    b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device, dtype=torch.float32), requires_grad=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> nn.ParameterList([W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">init_lstm_state</span><span class="params">(batch_size, num_hiddens, device)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (torch.zeros((batch_size, num_hiddens), device=device), </span><br><span class="line">            torch.zeros((batch_size, num_hiddens), device=device))</span><br></pre></td></tr></table></figure><pre><code>will use cpu</code></pre><h3 id="LSTM模型"><a href="#LSTM模型" class="headerlink" title="LSTM模型"></a>LSTM模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lstm</span><span class="params">(inputs, state, params)</span>:</span></span><br><span class="line">    [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c, W_hq, b_q] = params</span><br><span class="line">    (H, C) = state</span><br><span class="line">    outputs = []</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        I = torch.sigmoid(torch.matmul(X, W_xi) + torch.matmul(H, W_hi) + b_i)</span><br><span class="line">        F = torch.sigmoid(torch.matmul(X, W_xf) + torch.matmul(H, W_hf) + b_f)</span><br><span class="line">        O = torch.sigmoid(torch.matmul(X, W_xo) + torch.matmul(H, W_ho) + b_o)</span><br><span class="line">        C_tilda = torch.tanh(torch.matmul(X, W_xc) + torch.matmul(H, W_hc) + b_c)</span><br><span class="line">        C = F * C + I * C_tilda</span><br><span class="line">        H = O * C.tanh()</span><br><span class="line">        Y = torch.matmul(H, W_hq) + b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    <span class="keyword">return</span> outputs, (H, C)</span><br></pre></td></tr></table></figure><h3 id="训练模型-1"><a href="#训练模型-1" class="headerlink" title="训练模型"></a>训练模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">160</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">40</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line"></span><br><span class="line">d2l.train_and_predict_rnn(lstm, get_params, init_lstm_state, num_hiddens,</span><br><span class="line">                          vocab_size, device, corpus_indices, idx_to_char,</span><br><span class="line">                          char_to_idx, <span class="literal">False</span>, num_epochs, num_steps, lr,</span><br><span class="line">                          clipping_theta, batch_size, pred_period, pred_len,</span><br><span class="line">                          prefixes)</span><br></pre></td></tr></table></figure><pre><code>epoch 40, perplexity 211.056591, time 1.64 sec
 - 分开 我不的我 我不的我 我不的我 我不的我 我不的我 我不的我 我不的我 我不的我 我不的我 我不的我
 - 不分开 我不的我 我不的我 我不的我 我不的我 我不的我 我不的我 我不的我 我不的我 我不的我 我不的我
epoch 80, perplexity 65.091712, time 1.53 sec
 - 分开 我想你这你 我不要 我不 我不要 我不要 我不要 我不要 我不要 我不要 我不要 我不要 我不要 
 - 不分开 我想你你想你 我想想这你 我不要 我不要 我不要 我不要 我不要 我不要 我不要 我不要 我不要 
epoch 120, perplexity 17.263918, time 1.54 sec
 - 分开 我想你这生微 一天个对医药 我想这这样活 你天样 一直走 我想就好样 你不的节活 后知后觉 我该了
 - 不分开 我想你的生笑 你天  又你的我面听 一发抖 快给我抬起头 有话去对医药 说知后觉 我想了这节活 后
epoch 160, perplexity 3.906676, time 1.56 sec
 - 分开 你说的话不起 你学着碌的落 快什么 什什么 什么开有在留留 干什么 干什么 什么我有有片自 快使用
 - 不分开我 想要你 你想我 想要再 我不再再了快 说说去对医药箱 说说  想想了久了着? 我不想想想你 你你</code></pre><h3 id="简洁实现-1"><a href="#简洁实现-1" class="headerlink" title="简洁实现"></a>简洁实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens=<span class="number">256</span></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">160</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">40</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line"></span><br><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line">lstm_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens)</span><br><span class="line">model = d2l.RNNModel(lstm_layer, vocab_size)</span><br><span class="line">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure><pre><code>epoch 40, perplexity 1.028067, time 1.05 sec
 - 分开始我担 有AB血型的公老鼠 恍恍惚惚 是谁的脚步 银制茶壶 装蟑螂蜘蛛 辛辛苦苦 全家怕日出 白色蜡
 - 不分开 我好好好生活 我叫你已经很久 别想躲 说你眼睛看着我 别发抖 快给我抬起头 有话去对医药箱说 别怪
epoch 80, perplexity 1.021706, time 1.07 sec
 - 分开始我担 在小村外的溪边河口 默默的在等著我 家乡的爹娘早已苍老了轮廓 娘子我欠你太多 一壶好酒 再来
 - 不分开 我叫我的爱  你叫我学习你把你当榜样  好多的假像 妈妈常说乖听你爸的话  你叫我怎么跟你像 不要
epoch 120, perplexity 1.012656, time 1.02 sec
 - 分开始我攻 我的认真败给黑色幽默 走过了很多地方 我来到伊斯坦堡 就像是童话故事  有教堂有城堡 每天忙
 - 不分开不多难道 快攻抢篮板球 得分都靠我 你拿着球不投 又不会掩护我 选你这种队友 瞎透了我 说你说 分数
epoch 160, perplexity 1.010791, time 1.08 sec
 - 分开始我呼 在人有一切 真的可以 我想要将我的寂寞封闭 然后在这里 不限日期 然后将过去 慢慢温习 让我
 - 不分开 我叫我学爱你看棒球 想这样没担忧 唱着歌 一直走 我想就这样牵着你的手不放开 爱可不可以简简单单没</code></pre><h1 id="深度循环神经网络"><a href="#深度循环神经网络" class="headerlink" title="深度循环神经网络"></a>深度循环神经网络</h1><p><img src="https://cdn.kesci.com/upload/image/q5jk3z1hvz.png?imageView2/0/w/320/h/320" alt="Image Name"></p><p>$$<br>\boldsymbol{H}<em>t^{(1)} = \phi(\boldsymbol{X}_t \boldsymbol{W}</em>{xh}^{(1)} + \boldsymbol{H}<em>{t-1}^{(1)} \boldsymbol{W}</em>{hh}^{(1)} + \boldsymbol{b}<em>h^{(1)})\<br>\boldsymbol{H}_t^{(\ell)} = \phi(\boldsymbol{H}_t^{(\ell-1)} \boldsymbol{W}</em>{xh}^{(\ell)} + \boldsymbol{H}<em>{t-1}^{(\ell)} \boldsymbol{W}</em>{hh}^{(\ell)} + \boldsymbol{b}<em>h^{(\ell)})\<br>\boldsymbol{O}_t = \boldsymbol{H}_t^{(L)} \boldsymbol{W}</em>{hq} + \boldsymbol{b}_q<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn, optim</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"../input/"</span>)</span><br><span class="line"><span class="keyword">import</span> d2l_jay9460 <span class="keyword">as</span> d2l</span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line">(corpus_indices, char_to_idx, idx_to_char, vocab_size) = d2l.load_data_jay_lyrics()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">num_hiddens=<span class="number">256</span></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">160</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">40</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line"></span><br><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line"></span><br><span class="line">gru_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens,num_layers=<span class="number">2</span>)</span><br><span class="line">model = d2l.RNNModel(gru_layer, vocab_size).to(device)</span><br><span class="line">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure><pre><code>epoch 40, perplexity 1.779664, time 1.50 sec
 - 分开 我想多 一场默剧 你的完美主义 太彻底 让我连恨都难以下笔 将真心抽离写成日记 像是一场默剧 你的
 - 不分开想要再想 我不多 我有多烦恼  没有你烦我有多烦恼多难熬  没有你烦我有多烦恼多难熬  没有你烦我有
epoch 80, perplexity 1.017581, time 1.49 sec
 - 分开 我想一定是我听错弄错搞错 拜托 我想是你的脑袋有问题 随便说说 其实我早已经猜透看透不想多说 只是
 - 不分开 那场悲剧 是你完美演出的一场戏 宁愿心碎哭泣 再狠狠忘记 你爱过我的证据 让晶莹的泪滴 闪烁成回忆
epoch 120, perplexity 1.015036, time 1.50 sec
 - 分开 我有多难熬 我没有你烦 我有多烦恼  没有你烦我有多烦恼多难熬  穿过云层 我试著努力向你奔跑 爱
 - 不分开 我有多难熬 我没有你烦 我有多烦恼  没有你烦我有多烦恼多难熬  穿过云层 我试著努力向你奔跑 爱
epoch 160, perplexity 1.010326, time 1.51 sec
 - 分开 我有多难熬  没有你在 我有多难熬  没有你在我有多难熬多烦恼  没有你烦 我有多烦恼  没有你烦
 - 不分开 我有多难熬 我跟了这节奏 后知后觉 又过了一个秋 后知后觉 我该好好生活 我该好好生活 不知不觉 </code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">gru_layer = nn.LSTM(input_size=vocab_size, hidden_size=num_hiddens,num_layers=<span class="number">6</span>)</span><br><span class="line">model = d2l.RNNModel(gru_layer, vocab_size).to(device)</span><br><span class="line">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure><pre><code>epoch 40, perplexity 275.835469, time 3.79 sec
 - 分开                                                  
 - 不分开                                                  
epoch 80, perplexity 274.543441, time 3.88 sec
 - 分开                                                  
 - 不分开                                                  
epoch 120, perplexity 274.099434, time 3.97 sec
 - 分开                                                  
 - 不分开                                                  
epoch 160, perplexity 273.963849, time 4.07 sec
 - 分开                                                  
 - 不分开                                                  </code></pre><h1 id="双向循环神经网络"><a href="#双向循环神经网络" class="headerlink" title="双向循环神经网络"></a>双向循环神经网络</h1><p><img src="https://cdn.kesci.com/upload/image/q5j8hmgyrz.png?imageView2/0/w/320/h/320" alt="Image Name"></p><p>$$<br>\begin{aligned} \overrightarrow{\boldsymbol{H}}<em>t &amp;= \phi(\boldsymbol{X}_t \boldsymbol{W}</em>{xh}^{(f)} + \overrightarrow{\boldsymbol{H}}<em>{t-1} \boldsymbol{W}</em>{hh}^{(f)} + \boldsymbol{b}<em>h^{(f)})\<br>\overleftarrow{\boldsymbol{H}}_t &amp;= \phi(\boldsymbol{X}_t \boldsymbol{W}</em>{xh}^{(b)} + \overleftarrow{\boldsymbol{H}}<em>{t+1} \boldsymbol{W}</em>{hh}^{(b)} + \boldsymbol{b}<em>h^{(b)}) \end{aligned} $$<br>$$<br>\boldsymbol{H}_t=(\overrightarrow{\boldsymbol{H}}</em>{t}, \overleftarrow{\boldsymbol{H}}_t)<br>$$</p><p>$$<br>\boldsymbol{O}<em>t = \boldsymbol{H}_t \boldsymbol{W}</em>{hq} + \boldsymbol{b}_q<br>$$</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">num_hiddens=<span class="number">128</span></span><br><span class="line">num_epochs, num_steps, batch_size, lr, clipping_theta = <span class="number">160</span>, <span class="number">35</span>, <span class="number">32</span>, <span class="number">1e-2</span>, <span class="number">1e-2</span></span><br><span class="line">pred_period, pred_len, prefixes = <span class="number">40</span>, <span class="number">50</span>, [<span class="string">'分开'</span>, <span class="string">'不分开'</span>]</span><br><span class="line"></span><br><span class="line">lr = <span class="number">1e-2</span> <span class="comment"># 注意调整学习率</span></span><br><span class="line"></span><br><span class="line">gru_layer = nn.GRU(input_size=vocab_size, hidden_size=num_hiddens,bidirectional=<span class="literal">True</span>)</span><br><span class="line">model = d2l.RNNModel(gru_layer, vocab_size).to(device)</span><br><span class="line">d2l.train_and_predict_rnn_pytorch(model, num_hiddens, vocab_size, device,</span><br><span class="line">                                corpus_indices, idx_to_char, char_to_idx,</span><br><span class="line">                                num_epochs, num_steps, lr, clipping_theta,</span><br><span class="line">                                batch_size, pred_period, pred_len, prefixes)</span><br></pre></td></tr></table></figure><pre><code>epoch 40, perplexity 1.001314, time 0.98 sec
 - 分开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开
 - 不分开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开
epoch 80, perplexity 1.000417, time 0.98 sec
 - 分开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开
 - 不分开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开
epoch 120, perplexity 1.000207, time 0.99 sec
 - 分开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开
 - 不分开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开
epoch 160, perplexity 1.000124, time 0.94 sec
 - 分开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开始开
 - 不分开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开不开</code></pre><hr></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E7%AC%94%E8%AE%B0/" rel="tag"><i class="fa fa-tag"></i> 笔记</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a> <a href="/tags/pytorch/" rel="tag"><i class="fa fa-tag"></i> pytorch</a> <a href="/tags/GRU/" rel="tag"><i class="fa fa-tag"></i> GRU</a> <a href="/tags/LSTM/" rel="tag"><i class="fa fa-tag"></i> LSTM</a> <a href="/tags/deep-RNN/" rel="tag"><i class="fa fa-tag"></i> deep RNN</a> <a href="/tags/Bi-directional-RNN/" rel="tag"><i class="fa fa-tag"></i> Bi-directional RNN</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/" rel="tag"><i class="fa fa-tag"></i> 深度学习实战</a> <a href="/tags/python/" rel="tag"><i class="fa fa-tag"></i> python</a> <a href="/tags/jupyter-notebook/" rel="tag"><i class="fa fa-tag"></i> jupyter notebook</a></div><div class="post-widgets"><div id="needsharebutton-postbottom"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/2020/02/19/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E3%80%81%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/" rel="next" title="Task1.8 梯度消失、梯度爆炸以及Kaggle房价预测"><i class="fa fa-chevron-left"></i> Task1.8 梯度消失、梯度爆炸以及Kaggle房价预测</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"><a href="/2020/02/20/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/" rel="prev" title="Task1.10 机器翻译和数据集">Task1.10 机器翻译和数据集 <i class="fa fa-chevron-right"></i></a></div></div></footer></div><div><div><div style="text-align:center;color:#ccc;font-size:14px">------ 本文结束------</div></div></div></article><div class="post-spread"></div></div></div></div><div class="sidebar-toggle"><div class="sidebar-toggle-line-wrap"><span class="sidebar-toggle-line sidebar-toggle-line-first"></span> <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span> <span class="sidebar-toggle-line sidebar-toggle-line-last"></span></div></div><aside id="sidebar" class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">文章目录</li><li class="sidebar-nav-overview" data-target="site-overview-wrap">站点概览</li></ul><section class="site-overview-wrap sidebar-panel"><div class="site-overview"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><a href="/"><img class="site-author-image" itemprop="image" src="/uploads/avatar.png" alt="Seventeen Chen"></a><p class="site-author-name" itemprop="name">Seventeen Chen</p><p class="site-description motion-element" itemprop="description">This is my blog</p></div><nav class="site-state motion-element"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">14</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/index.html"><span class="site-state-item-count">3</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/index.html"><span class="site-state-item-count">54</span> <span class="site-state-item-name">标签</span></a></div></nav><div class="feed-link motion-element"><a href="/atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/SeventeenChen" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:cherry.oldchen@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a> </span><span class="links-of-author-item"><a href="https://twitter.com/SeventeenChen17" target="_blank" title="Twitter"><i class="fa fa-fw fa-twitter"></i>Twitter</a></span></div></div></section><section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active"><div class="post-toc"><div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#GRU"><span class="nav-number">1.</span> <span class="nav-text">GRU</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#载入数据集"><span class="nav-number">1.0.1.</span> <span class="nav-text">载入数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#初始化参数"><span class="nav-number">1.0.2.</span> <span class="nav-text">初始化参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GRU模型"><span class="nav-number">1.0.3.</span> <span class="nav-text">GRU模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#训练模型"><span class="nav-number">1.0.4.</span> <span class="nav-text">训练模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#简洁实现"><span class="nav-number">1.0.5.</span> <span class="nav-text">简洁实现</span></a></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#LSTM"><span class="nav-number">2.</span> <span class="nav-text">LSTM</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#初始化参数-1"><span class="nav-number">2.0.1.</span> <span class="nav-text">初始化参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LSTM模型"><span class="nav-number">2.0.2.</span> <span class="nav-text">LSTM模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#训练模型-1"><span class="nav-number">2.0.3.</span> <span class="nav-text">训练模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#简洁实现-1"><span class="nav-number">2.0.4.</span> <span class="nav-text">简洁实现</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#深度循环神经网络"><span class="nav-number">3.</span> <span class="nav-text">深度循环神经网络</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#双向循环神经网络"><span class="nav-number">4.</span> <span class="nav-text">双向循环神经网络</span></a></li></div></div></section></div></aside></div></main><footer id="footer" class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-user"></i> </span><span class="author" itemprop="copyrightHolder">Seventeen Chen</span></div><div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div><span class="post-meta-divider">|</span><div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div></div></footer><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span id="scrollpercent"><span>0</span>%</span></div><div id="needsharebutton-float"><span class="btn"><i class="fa fa-share-alt" aria-hidden="true"></i></span></div></div><script type="text/javascript">"[object Function]"!==Object.prototype.toString.call(window.Promise)&&(window.Promise=null)</script><script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script><script type="text/javascript" src="true"></script><script type="text/javascript" src="true"></script><script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script><script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script><script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script><script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script><link rel="stylesheet" href="/lib/needsharebutton/needsharebutton.css"><script src="/lib/needsharebutton/needsharebutton.js"></script><script>pbOptions={},pbOptions.iconStyle="default",pbOptions.boxForm="horizontal",pbOptions.position="bottomCenter",pbOptions.networks="Weibo,Wechat,Douban,QQZone,Twitter,Facebook",new needShareButton("#needsharebutton-postbottom",pbOptions),flOptions={},flOptions.iconStyle="box",flOptions.boxForm="horizontal",flOptions.position="middleRight",flOptions.networks="Weibo,Wechat,Douban,QQZone,Twitter,Facebook",new needShareButton("#needsharebutton-float",flOptions)</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });</script><script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type="text/javascript" src="/lib/clipboard/clipboard.js"></script><script type="text/javascript" src="/js/src/custom.js"></script></body></html><!-- rebuild by neat -->